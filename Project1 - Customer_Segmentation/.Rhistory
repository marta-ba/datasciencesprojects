AP_New$Teenhome = replace(AP_New$Teenhome, AP_New$Teenhome == "0", "Sin Adolescentes")
AP_New$Teenhome = replace(AP_New$Teenhome, AP_New$Teenhome == "1", "Con Adolescentes")
Teenhome_Num_Levels = distinct_function_count (AP_New,"Teenhome")
formattable(Teenhome_Num_Levels)
#Resultado:
# --> Sin Adolescentes: 51.7 %
# --> Con Adolescentes: 48.3 %
AP_New$Teenhome = as.factor(AP_New$Teenhome)
#Pasamos a la variable complain que indica si el cliente se quejó en los últimos 2 años o no
Complain_Num_Levels = distinct_function_count (AP_New,"Complain")
formattable(Complain_Num_Levels)
# Mantengo los datos como están, a pesar de que el % de registros del nivel "1" es sumamente bajo.
# Acabo con la variable Response:
Response_Num_Levels = distinct_function_count (AP_New,"Response")
formattable(Response_Num_Levels)
# Mantengo los datos como están.
#Comprueba ahora si hay alguna variable categórica que tenga datos ausentes:
summary(AP_New)
#Se comprueba que no existen datos ausentes en las variables categóricas
# Una vez analizadas las variables, hechas las imputacioens y las agrupaciones de los niveles, pasamos al análisis descriptivo de las mismas a través de las gráficos de barras:
# El análsis descriptivo de las variables categóricas lo llevo a cabo por medio de la definición de dos funciones:
# 1) Que me genere un gráfico de barras Que me permita observar de manera clara como están distribuidos cada uno de los niveles de cada nivel en cada variable y me los ordene en el eje X decrecientemente y
# 2) Una gráfica de barras que me permita observar la distribución que tiene cada nivel de la variable target en cada nivel de cada una de las variables.
bar_plot <- function(data, var){
var <- as.symbol(var)
g <- ggplot(data, aes(x = !!var)) +
geom_bar(fill="lightblue",mapping=aes(x=reorder(!!var,!!var,length,decreasing=T))) +
geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, color = "red")
return (g)
}
bar_target_var_plot <- function(data, var, target_var = "Num_Compras_Totales") {
var <- as.symbol(var)
target_var <- as.symbol(target_var)
# Se saca el % por nivel de variable categórica en cada variable
plot_data <- data %>%
count(!!var, !!target_var) %>%
group_by(!!var) %>%
mutate(percent = n / sum(n))
g <- ggplot(plot_data, aes(x = reorder(!!var, !!var, length, decreasing = TRUE), y = percent, fill = !!target_var)) +
geom_bar(stat = "identity") +
geom_label(aes(label = scales::percent(percent)),
position = position_stack(vjust = 0.5), color = "red",
show.legend = FALSE) +
ylab("Porcentaje") +
xlab ("Variable")
return(g)
}
BarP_Education <- bar_plot(AP_New, "Education")
BarPT_Education <- bar_target_var_plot(AP_New, "Education")
BarP_Education
BarPT_Education
# Varias conclusiones interesantes en este gráfico:
# --> En el grupo de "Non University Degree": Más de un 60% de los clientes realizan menos de 8 compras.
# --> En el resto de grupos (los clientes con estudios) si que es cierto que el número de compras es más homogeneo.
# --> En todos los grupos, en nivel "Mayor o igual a 16" supera a los "Entre 8 y 15"
# Procedo con el mismo análisis en la variable de estado civil
BarP_Marital_Status_woNA <- bar_plot(AP_New, "Marital_Status_woNA")
BarPT_Marital_Status_woNA <- bar_target_var_plot(AP_New, "Marital_Status_woNA")
BarP_Marital_Status_woNA
BarPT_Marital_Status_woNA
# --> Realmente ambos grupos se comportan de manera muy similar
# --> En todos los grupos, en nivel "Mayor o igual a 16" supera a los "Entre 8 y 15"
#Paso a estudiar los clientes en función de si tienen niños en casa:
BarP_Kidhome <- bar_plot(AP_New, "Kidhome")
BarPT_Kidhome <- bar_target_var_plot(AP_New, "Kidhome")
BarP_Kidhome
BarPT_Kidhome
# Dos conclusiones importantes:
#1) Los clientes con grupos con niños suelen comprar mucho menos.
#2) os clientes sin niños compran en más de un 50% más de 16 compras!!!
#Paso a estudiar los clientes en función de si tienen adolesentes en casa:
BarP_Teenhome <- bar_plot(AP_New, "Teenhome")
BarPT_Teenhome <- bar_target_var_plot(AP_New, "Teenhome")
BarP_Teenhome
BarPT_Teenhome
# Curioso: En el grupo de los clientes que no tienen adolescentes en casa, a diferencia de lo que se pudiera sospechar, el nivel mayoritario en la variable objetivo es: "Menor o igual a 7"
#Paso a estudiar los clientes que se han quejado o no en los últimos dos años:
BarP_Complain<- bar_plot(AP_New, "Complain")
BarPT_Complain <- bar_target_var_plot(AP_New, "Complain")
BarP_Complain
BarPT_Complain
#Principal conclusión: Entre los que se quejaron, compraron poco, SEGURAMENTE DEJARIAN DE COMPRAR HACE TIEMPO!
#Por último estudiamos la variable "Response":
BarP_Response<- bar_plot(AP_New, "Response")
BarPT_Response <- bar_target_var_plot(AP_New, "Response")
BarP_Response
BarPT_Response
#Principal conclusión: En el grupo de los que aceptaron la última campaña, el número de comprar mayoritario es "Mayor o igual a 16"
# Tras la realización del análisis descriptivo, se realiza la imputación de valores faltantes de las variables numéricas
# Pienso que puede ser interesante conocer la correlación únicamente entre las variables de los montos de los prouctos, para distinguir tipos de productos.
#Parto del dataframe "normalizado":
df_num_log <- as.data.frame(AP_New)
df_prod = df_num_log %>% select (MntWines,MntFruits,MntMeatProducts,MntFishProducts,MntSweetProducts,MntGoldProds)
#Calculo de nuevo la tabla de correlaciones:
correlacion_pearson_df_log = cor(df_prod, use = "pairwise.complete.obs")
corrplot(correlacion_pearson_df_log,
tl.cex=0.5,
tl.offset = 0.6,
type="upper",
method = "number",
addCoef.col="grey",
order = "AOE",
number.cex=1)
#Conclusiones: No saco conclusiones más interesantes que la que ya conocía: Los montos de carne y vino están muy relacionados, pero poco mas, dado que el resto de variables tienen una covarianza alta (todas menor de 0.6)
# Durante el análisis descriptivo de las variables cuantitativas se ha visto que existe una fuerte correlación lineal entre las variables "MntWines", "MntMeatProducts" e `Incomes`. Así, se plantea una regresión lineal con estas tres variables. Solo MntMeatProducts carece de datos ausentes.
#Creo que lo adecuado sería hacerlo paso por paso, es decir, sacar los valores de vino por medio de los de la carne, mediante una regresión lineal simple y luego los de los ingresos por medio de otra regresión lineal múltiple.
# De modo que:
Vino_non_nulls_df <- AP_New %>%
select(ID, MntWines , MntMeatProducts) %>%
filter(!is.na(MntWines))
lm_vino <- lm(MntWines ~ MntMeatProducts, data=Vino_non_nulls_df %>% select(-ID))
summary(lm_vino)
#El R^2 me sale muy bajo (0.32), sin embargo observo del siguiente gráfico que efecivimanete existe una relación entre ambas variables:
xy_vino_carne = xyplot(MntWines ~ MntMeatProducts, type="p", pch=16,
auto.key=list(border=TRUE), par.settings=simpleTheme(pch=16),
scales=list(x=list(relation='same'), y=list(relation='same')), data=AP_New)
xy_vino_carne
# Supongo que será por los datos outliers, por lo que trato de elimnarlos. Elimino los datos outliers de las dos variables:
Vino_non_nulls_non_Outl_df = outliers_univariantes (Vino_non_nulls_df,"MntWines")
Vino_Carne_non_nulls_non_Outl_df = outliers_univariantes (Vino_non_nulls_non_Outl_df,"MntMeatProducts")
lm_vino_2 <- lm(MntWines ~ MntMeatProducts , data=Vino_Carne_non_nulls_non_Outl_df %>% select(-ID))
summary(lm_vino_2)
#El R^2 mejora pero no mucho.: 0,3971 en este caso.
#No obstante, dado que las variables de la regresión son significativas al 99% y que la covarianza salia de 0.81, procedo a la regresión lineal simple de estas variables para la imputación de los valores NAs de la variable "MntWines":
AP_New <- AP_New %>%
mutate(Vino_IMPUTE = round(predict.lm(lm_vino_2, newdata = AP_New %>% select(MntMeatProducts)),0))
AP_New <- mutate(AP_New, MntWines := coalesce(MntWines, Vino_IMPUTE))
AP_New$Vino_IMPUTE <- NULL
summary(AP_New)
#Como comentaba anteriormente, una vez imputado los datos del vino, procedo a imputar los datos faltantes de los ingresos:
#En primer lugar voy a crear un nuevo dataset eliminando los valores NAs de la variable Income:
Ingresos_non_nulls_df <- AP_New %>%
select(ID, Income, MntWines , MntMeatProducts) %>%
filter(!is.na(Income))
#Ahora hago la regresión con este nuevo dataset:
RegModel_Ingresos_CarneyVino <- lm(Income~MntMeatProducts+MntWines,
data=Ingresos_non_nulls_df)
summary(RegModel_Ingresos_CarneyVino)
#R^2 vuelve a no ser muy alto: 0.529, pero los coeficientes son significativos... Analizo los datos por medio de gráfico XY:
xy_Ingresos_CarneyVino = xyplot(MntMeatProducts + MntWines ~ Income, type="p", pch=16,
auto.key=list(border=TRUE), par.settings=simpleTheme(pch=16),
scales=list(x=list(relation='same'), y=list(relation='same')),
data=Ingresos_non_nulls_df)
xy_Ingresos_CarneyVino
#Se observa una fuerte relación entre las 3 variables. Como anteriormente, elimino outliers de las variables para tratar de mejorar R^2:
Ingresos_non_nulls_non_Outl_df = outliers_univariantes (Ingresos_non_nulls_df,"Income")
Ingresos_Vino_non_nulls_non_Outl_df = outliers_univariantes (Ingresos_non_nulls_non_Outl_df,"MntWines")
Ingresos_Vino_Carne_non_nulls_non_Outl_df = outliers_univariantes (Ingresos_Vino_non_nulls_non_Outl_df,"MntMeatProducts")
RegModel_Ingresos_CarneyVino_2 <- lm(Income~MntMeatProducts+MntWines,
data=Ingresos_Vino_Carne_non_nulls_non_Outl_df)
summary(RegModel_Ingresos_CarneyVino_2)
#R^2 = 0.6164, pero todos los coeficientes son significativos al 99%. Represento gráfico de puntos con recta de regresión:
Coef1 = RegModel_Ingresos_CarneyVino_2[["coefficients"]][["MntMeatProducts"]]
Coef2 = RegModel_Ingresos_CarneyVino_2[["coefficients"]][["MntWines"]]
plot(x=Coef1*Ingresos_Vino_Carne_non_nulls_non_Outl_df$MntMeatProducts+Coef2*Ingresos_Vino_Carne_non_nulls_non_Outl_df$MntWines, y=Ingresos_Vino_Carne_non_nulls_non_Outl_df$Income, main="Ingresos vs Carne + Vino", pch=20, col = "red", xlab="Variables independientes", ylab="Ingresos")
points(Coef1*Ingresos_Vino_Carne_non_nulls_non_Outl_df$MntMeatProducts+Coef2*Ingresos_Vino_Carne_non_nulls_non_Outl_df$MntWines, fitted(RegModel_Ingresos_CarneyVino_2), col='blue', pch=20, type = "l")
#En el gráfico anterior se observa la fuerte correlación que hay entre las variables: MntMeatProducts y MntWines con Income.
#Dada esta regresión, porcedo a imputar los valores ausentes de la variable "Income"
AP_New <- AP_New %>%
mutate(Ingresos_IMPUTE = predict.lm(RegModel_Ingresos_CarneyVino_2, newdata = AP_New %>% select(MntMeatProducts,MntWines)))
AP_New <- mutate(AP_New, Income := coalesce(Income, Ingresos_IMPUTE))
AP_New$Ingresos_IMPUTE<- NULL
summary(AP_New)
#Imputo los valores NAs a los montantes de los productos MntFruits, MntFishProducts y MntSweetProducts por medio de las medianas de los demás valores existentes eliminando los outliers. De modo que:
#Creo una función para sacarla mediana de cada variable obviando los outliers:
Imputación_Var <- function(data,vari){
vari <- as.symbol(vari)
NO_Out = outliers_univariantes(data,var=vari)
mediana_df <- NO_Out %>%
select(!!vari) %>%
filter(!is.na(!!vari)) %>%
summarize(MEDIANA = median(!!vari))
return (mediana_df)
}
#Voy impuntando con las medianas obtenidas:
Mediana_Fruta = Imputación_Var (AP_New, "MntFruits")
AP_New <- mutate(AP_New, MntFruits := coalesce(MntFruits, Mediana_Fruta$MEDIANA))
Mediana_Pescado = Imputación_Var (AP_New, "MntFishProducts")
AP_New <- mutate(AP_New, MntFishProducts := coalesce(MntFishProducts, Mediana_Pescado$MEDIANA))
Mediana_Dulce = Imputación_Var (AP_New, "MntSweetProducts")
AP_New <- mutate(AP_New, MntSweetProducts := coalesce(MntSweetProducts, Mediana_Dulce$MEDIANA))
#Chequeo que no queda ningun dato ausente en estas variables:
summary(AP_New)
#Por tanto, solo quedan vallores NAs en la variable NumWebVisitsMonth, la cual procedo a analaizar:
# Veamos como se relacionan ambas variables:
df_web_imp <- Análisis_Personalidad %>%  filter(!is.na(NumWebPurchases) & !is.na(NumWebVisitsMonth)) %>% select(ID,NumWebPurchases,NumWebVisitsMonth) %>%
group_by(NumWebPurchases) %>%
summarize(VISITAS_MEDIA = round(mean(NumWebVisitsMonth),0))
AP_New_Prueba = AP_New
AP_New_Prueba <-  left_join(Análisis_Personalidad, df_web_imp)
AP_New_Prueba <- mutate(AP_New_Prueba, NumWebVisitsMonth := coalesce(NumWebVisitsMonth, VISITAS_MEDIA))
AP_New_Prueba$VISITAS_MEDIA <- NULL
summary(AP_New_Prueba)
#Siguen quedando 9 datos ausentes, analizo la variable en su conjunto:
df_web_imp_2 = distinct_function_count(AP_New_Prueba,"NumWebVisitsMonth")
formattable (df_web_imp_2 )
#Los 9 datos NAs, los imputo al nivel mayoritario, este es, NumWebVisitsMonth = 7, de modo que:
AP_New_Prueba$NumWebVisitsMonth <- replace(AP_New$NumWebVisitsMonth, is.na(AP_New$NumWebVisitsMonth),7)
#Ahora traslado esta variable a mi dataset de trabajo, este es, AP_New. De modo que:
AP_New = cbind (AP_New,Visitas_web=AP_New_Prueba$NumWebVisitsMonth)
#Elimino la variable antigua NumWebVisitsMonth de AP_New
AP_New$NumWebVisitsMonth = NULL
#Se comprueba que ya no tenemos datos NAs en todo nuestro dataset:
summary(AP_New)
#Antes de analizar las instancias anómalas y en base a lo estudiado, voy a simplificar el dataset:
# En primer lugar voy a eliminar las variables Dt_Customer y Recency, agrupandolas en otra llamada "Dias_Registro_ult_compra" que me va a calcular el número de días que en cliente ha tenido su usuario activo:
Fecha = as.Date(AP_New$Dt_Customer, format = "%d-%m-%Y")
Dias_Registro_ult_compra = as.Date("28-02-2024",format = "%d-%m-%Y") - Fecha - AP_New$Recency
#Incluyo esta variable en el data set y elimino: Dt_Customer y Recency
AP_New = cbind(AP_New,Días_Registro_ult_compra = Dias_Registro_ult_compra)
#Paso los valores de esta nueva variable a numeros:
AP_New$Días_Registro_ult_compra = as.numeric(AP_New$Días_Registro_ult_compra)
#Analizo esta nueva variable por medio de las funciones anteriores que me permitían obtener los hitogramas y los diagramas de cajas:
# Elimino de nuevo los Outliers:
Días_Registro_ult_compra_wo_outliers_NCT = outliers_univariantes (AP_New,"Días_Registro_ult_compra")
#Extraigo las gráficas
Días_Registro_ult_compra_hvtp_log = histogram_var_target_plot_NCT (Días_Registro_ult_compra_wo_outliers_NCT,"Días_Registro_ult_compra")
Días_Registro_ult_compra_bvtp_log = boxplot_var_target_plot_NCT (Días_Registro_ult_compra_wo_outliers_NCT,"Días_Registro_ult_compra")
grid.arrange(Días_Registro_ult_compra_hvtp_log, Días_Registro_ult_compra_bvtp_log,nrow = 2, ncol=1)
#Del hitograma obtengo una gráfica que aparentemente es "normal". De este gráfico se aprecia, como no puede ser de otro modo, como, conforme aumenta los días de usuario activo, el nivel "mayor o igual que 16" va en aumento.
#Del diagrama de cajas observo que: los intervalos intercuartilicos de "mayor o igual que 16" y "entre 8 y 15" se parecen mucho, quizás la mediana es algo mayor en el primer nivel.
AP_New$Dt_Customer = NULL
AP_New$Recency = NULL
#Por otra, parte, se elimina del dataset también la variable "Complain", dado que, no aportaba gran información debido la gran diferencia de registros entre ambos niveles:
AP_New$Complain = NULL
## Inspección de datos anómalos
#A lo largo del análisis realizado se ha observado de forma univariante la existencia de varios valores atípicos. Se plantea el uso del algoritmo **Isolation Forest** para identificar qué instancias pueden ser consideradas como anómalas.
# Comenzamos creando una copia del dataset AP_New y eliminando la variable ID:
df_anomaly <- AP_New
df_anomaly$ID <- NULL #Se quita para el calculo del score para meter un sesgo.
# Se establece una semilla para la generación de números aleatorios:
set.seed(123)
# Se utiliza la fiunción Isolation Forest para detectar anomalías identificando observaciones inusuales o atípicas en un conjunto de dato
clf <- isoforest <- isolationForest$new(sample_size = as.integer(nrow(AP_New)),
num_trees = 100,
seed = 123
)
# Se ajusta el modelo Isolation Forest al conjunto de datos df_anomaly
clf$fit(dataset = df_anomaly)
#Se calcula el puntaje de anomalía para cada muestra en el conjunto de datos original (AP_New). El puntaje de anomalía indica qué tan inusual es cada muestra.
prob_anomaly <- clf$predict(data = AP_New)$anomaly_score
#e agrega una nueva columna llamada score al dataframe AP_New, que contiene los puntajes de anomalía calculados previamente.
AP_New <- mutate(AP_New, score = prob_anomaly)
#Empleamos un histograma para fijar un umbral para identificar instancias anómalas.
ggplot(AP_New, aes(x=score)) + geom_histogram(bins=50)
summary (AP_New$score)
#En este caso, dado que el 3er cuartil es 0,5734, se elige un umbral de 0.58. Así, si el score es superior a dicho valor se clasifica como instancia anómala.
threshold <- 0.58
# Se crea una nueva variable ("Cluster") que será 1 si la instancia es anómala y 0 si no lo es. Se elimna la variable "score":
AP_New <- mutate(AP_New , cluster = if_else(score > threshold, 1, 0))
AP_New$cluster <- as.factor(AP_New$cluster)
AP_New$score <- NULL
#Veamos como mejoran las variables numéricas mediant diagrama de cajas:
Bvtp_cluster_Year_Birth = boxplot_var_target_plot_NCT (AP_New,"Year_Birth","cluster")
Bvtp_cluster_Income = boxplot_var_target_plot_NCT (AP_New,"Income","cluster")
Bvtp_cluster_MntWines = boxplot_var_target_plot_NCT (AP_New,"MntWines","cluster")
Bvtp_cluster_MntFruits = boxplot_var_target_plot_NCT (AP_New,"MntFruits","cluster")
Bvtp_cluster_MntMeatProducts = boxplot_var_target_plot_NCT (AP_New,"MntMeatProducts","cluster")
Bvtp_cluster_MntFishProducts = boxplot_var_target_plot_NCT (AP_New,"MntFishProducts","cluster")
Bvtp_cluster_MntSweetProducts = boxplot_var_target_plot_NCT (AP_New,"MntSweetProducts","cluster")
Bvtp_cluster_MntGoldProds = boxplot_var_target_plot_NCT (AP_New,"MntGoldProds","cluster")
Bvtp_cluster_Días_Registro_ult_compra = boxplot_var_target_plot_NCT (AP_New,"Días_Registro_ult_compra","cluster")
grid.arrange(Bvtp_cluster_Year_Birth, Bvtp_cluster_Income,nrow = 2, ncol=1)
grid.arrange(Bvtp_cluster_MntWines, Bvtp_cluster_MntFruits, nrow = 2, ncol=1)
grid.arrange(Bvtp_cluster_MntMeatProducts,Bvtp_cluster_MntFishProducts,nrow = 2, ncol=1)
grid.arrange(Bvtp_cluster_MntSweetProducts, Bvtp_cluster_MntGoldProds, nrow = 2, ncol=1)
Bvtp_cluster_Días_Registro_ult_compra
summary(AP_New)
#316 instancias son declaradas como análogas un threshold de 0.58.
#1924instancias son declaradas como NO análogas un threshold de 0.58.
#Comparamos medias para cada variable entre instancias anómalas e instancias no anómalas:
media_cluster_AP <- AP_New %>%
select(cluster,
Year_Birth,
Income,
MntWines,
MntFruits,
MntMeatProducts,
MntFishProducts,
MntSweetProducts,
MntGoldProds,
Días_Registro_ult_compra) %>%
group_by(cluster) %>%
summarize(N = n(),
MEAN_Year_Birth = mean(Year_Birth),
MEAN_Income = mean(Income),
MEAN_MntWines = mean(MntWines),
MEAN_MntFruits = mean(MntFruits),
MEAN_MntMeatProducts = mean(MntMeatProducts),
MEAN_MntFishProducts = mean(MntFishProducts),
MEAN_MntSweetProducts = mean(MntSweetProducts),
MEAN_MntGoldProds = mean(MntGoldProds),
MEAN_Días_Registro_ult_compra = mean(Días_Registro_ult_compra)
)
formattable(media_cluster_AP)
# Se observa una gran diferencia en las siguientes variables relacionadas con los productos, dado que contienen un gran número de outliers.
# Finalmente, eliminamos las instancias marcadas como anómalas:
AP_Final <- AP_New %>% filter(cluster == 0)
AP_Final$cluster = NULL
formattable(distinct_function_count(AP_Final, "Num_Compras_Totales"))
summary(AP_Final)
View(AP_Final)
#`ADASYN` y `MUESTREO DEL CUBO` son los dos métodos que vamos a emplear para equilibrar la muestra.
#Elimino l variable ID:
AP_Final$ID <- NULL
#Utilizaremos el muestreo del cubo para obtener una muestra de la clase mayoritaria (**n=877**) y el Adasyn para realizar oversampling sobre las clases minoritarias.
#- Paso 1: undersampling clase mayoritaria mediante el muestreo del cubo
#En primer lugar, hay que tener en cuenta las variables de equilibrio (en este caso las variables cualitativas).
# Datos donde efectuamos selección de las comprar menores o iguales a 7:
df_7 <- AP_Final %>% filter(Num_Compras_Totales == "Menor o igual a 7")
# Número de compras menores o iguales a 7
df_menor_8 <- nrow(df_7)
# Creamos las variables indicadores para cada una de las variables de equilibrio.
# Variable que vale 1 en todas las partes (para comprobar la estimación del tamaño poblacional)
UNO = rep(1, df_menor_8)
# Variables cuantitativas
X1 <- df_7 [ ,c("Year_Birth", "Income", "MntWines", "MntFruits", "MntMeatProducts","MntFishProducts","MntSweetProducts","MntGoldProds","Días_Registro_ult_compra")]
# Variables cualitativas - creación de las variables indicadoras
X2 <- disjunctive(df_7$Education)
colnames(X2) <- levels(df_7$Education)
X3 <- disjunctive(df_7$Marital_Status_woNA)
colnames(X3) <- levels(df_7$Marital_Status_woNA)
X4 <- disjunctive(df_7$Kidhome)
colnames(X4) <- levels(df_7$Kidhome)
X5 <- disjunctive(df_7$Teenhome)
colnames(X5) <- levels(df_7$Teenhome)
X6 <- disjunctive(df_7$Response)
colnames(X6) <- levels(df_7$Response)
# Matriz de diseño
X <- as.matrix( cbind( UNO, X1, X2, X3, X4, X5 ) )
# Tamaño de la muestra
sample_df_7 <- 640
# Probabilidades de inclusión
pik = rep( sample_df_7 / df_menor_8, df_menor_8 )
# extracción de la muestra
set.seed(123)
s = samplecube(X, pik, method = 2, order = 1, comment = FALSE)
# Generación de fichero resultante
sample_df_final = cbind( df_7, s )
sample_df_final <- sample_df_final[ sample_df_final$s == 1, ]
sample_df_final$s  <- NULL
summary(sample_df_final)
#Calidad de la muestra obtenida. Se analiza del siguiente modo:
nrow(AP_New)/3
nrow(AP_Final)/3
Totales <- apply(X, 2, sum)
Horvitz.Thompson <- apply(X * s / pik, 2, sum)
calidad <- cbind.data.frame(Totales, Horvitz.Thompson)
calidad$Desv.Abs. <- round(calidad$Totales - calidad$Horvitz.Thompson, 2)
calidad$Desv.Rel. <- round((calidad$Totales / calidad$Horvitz.Thompson - 1) *100, 2)
formattable(calidad)
df_8_15 <- AP_Final %>% filter(Num_Compras_Totales == "Entre 8 y 15 (ambos incluidos)")
df_8_15 <- AP_Final %>% filter(Num_Compras_Totales == "Entre 8 y 15 (ambos incluidos)")
df_sample <- bind_rows(sample_df_final, df_8_15)
df_8_15 <- AP_Final %>% filter(Num_Compras_Totales == "Entre 8 y 15 (ambos incluidos)")
df_16 <- AP_Final %>% filter(Num_Compras_Totales == "Mayor o igual a 16")
df_sample <- bind_rows(sample_df_final, df_8_15,df_16)
df_equilibrio_cube_adasyn <- AdasynClassif(Num_Compras_Totales ~ ., df_sample,
k=6, # número de vecinos
baseClass = c("Menor o igual a 7"), # identifica la clase mayoritaria
dist = "HEOM") # métrica para tener en cuenta variables numéricas y nominales
formattable(distinct_function_count(df_equilibrio_cube_adasyn, "Num_Compras_Totales"))
df_8_15 <- AP_Final %>% filter(Num_Compras_Totales == "Entre 8 y 15 (ambos incluidos)")
df_16 <- AP_Final %>% filter(Num_Compras_Totales == "Mayor o igual a 16")
df_sample <- bind_rows(sample_df_final, df_8_15,df_16)
# PASO 3: incrementamos las muestras de las clases minoritarias haciendo uso del algoritmo Adasyn
df_equilibrio_cube_adasyn <- AdasynClassif(Num_Compras_Totales ~ ., df_sample,
k=10, # número de vecinos
baseClass = c("Menor o igual a 7"), # identifica la clase mayoritaria
dist = "HEOM") # métrica para tener en cuenta variables numéricas y nominales
formattable(distinct_function_count(df_equilibrio_cube_adasyn, "Num_Compras_Totales"))
df_equilibrio_cube_adasyn <- AdasynClassif(Num_Compras_Totales ~ ., df_sample,
k=20, # número de vecinos
baseClass = c("Menor o igual a 7"), # identifica la clase mayoritaria
dist = "HEOM") # métrica para tener en cuenta variables numéricas y nominales
formattable(distinct_function_count(df_equilibrio_cube_adasyn, "Num_Compras_Totales"))
# Por la tabla anterior, se demuestra que la muestra queda equilibrada
df_equilibrio_cube_adasyn <- AdasynClassif(Num_Compras_Totales ~ ., df_sample,
k=200, # número de vecinos
baseClass = c("Menor o igual a 7"), # identifica la clase mayoritaria
dist = "HEOM") # métrica para tener en cuenta variables numéricas y nominales
formattable(distinct_function_count(df_equilibrio_cube_adasyn, "Num_Compras_Totales"))
# Por la tabla anterior, se demuestra que la muestra queda equilibrada
df_equilibrio_cube_adasyn <- AdasynClassif(Num_Compras_Totales ~ ., df_sample,
k=20, # número de vecinos
baseClass = c("Menor o igual a 7"), # identifica la clase mayoritaria
dist = "HEOM") # métrica para tener en cuenta variables numéricas y nominales
formattable(distinct_function_count(df_equilibrio_cube_adasyn, "Num_Compras_Totales"))
# Por la tabla anterior, se demuestra que la muestra queda equilibrada
#`ADASYN` y `MUESTREO DEL CUBO` son los dos métodos que vamos a emplear para equilibrar la muestra.
#Elimino l variable ID:
AP_Final$ID <- NULL
#Utilizaremos el muestreo del cubo para obtener una muestra de la clase mayoritaria (**n=877**) y el Adasyn para realizar oversampling sobre las clases minoritarias.
#- Paso 1: undersampling clase mayoritaria mediante el muestreo del cubo
#En primer lugar, hay que tener en cuenta las variables de equilibrio (en este caso las variables cualitativas).
# Datos donde efectuamos selección de las comprar menores o iguales a 7:
df_7 <- AP_Final %>% filter(Num_Compras_Totales == "Menor o igual a 7")
# Número de compras menores o iguales a 7
df_menor_8 <- nrow(df_7)
# Creamos las variables indicadores para cada una de las variables de equilibrio.
# Variable que vale 1 en todas las partes (para comprobar la estimación del tamaño poblacional)
UNO = rep(1, df_menor_8)
# Variables cuantitativas
X1 <- df_7 [ ,c("Year_Birth", "Income", "MntWines", "MntFruits", "MntMeatProducts","MntFishProducts","MntSweetProducts","MntGoldProds","Días_Registro_ult_compra")]
# Variables cualitativas - creación de las variables indicadoras
X2 <- disjunctive(df_7$Education)
colnames(X2) <- levels(df_7$Education)
X3 <- disjunctive(df_7$Marital_Status_woNA)
colnames(X3) <- levels(df_7$Marital_Status_woNA)
X4 <- disjunctive(df_7$Kidhome)
colnames(X4) <- levels(df_7$Kidhome)
X5 <- disjunctive(df_7$Teenhome)
colnames(X5) <- levels(df_7$Teenhome)
X6 <- disjunctive(df_7$Response)
colnames(X6) <- levels(df_7$Response)
# Matriz de diseño
X <- as.matrix( cbind( UNO, X1, X2, X3, X4, X5 ) )
# Tamaño de la muestra
sample_df_7 <- 636
# Probabilidades de inclusión
pik = rep( sample_df_7 / df_menor_8, df_menor_8 )
# extracción de la muestra
set.seed(123)
s = samplecube(X, pik, method = 2, order = 1, comment = FALSE)
# Generación de fichero resultante
sample_df_final = cbind( df_7, s )
sample_df_final <- sample_df_final[ sample_df_final$s == 1, ]
sample_df_final$s  <- NULL
summary(sample_df_final)
#Calidad de la muestra obtenida. Se analiza del siguiente modo:
Totales <- apply(X, 2, sum)
Horvitz.Thompson <- apply(X * s / pik, 2, sum)
calidad <- cbind.data.frame(Totales, Horvitz.Thompson)
calidad$Desv.Abs. <- round(calidad$Totales - calidad$Horvitz.Thompson, 2)
calidad$Desv.Rel. <- round((calidad$Totales / calidad$Horvitz.Thompson - 1) *100, 2)
formattable(calidad)
# La mayor desviaciónn relativa se tiene con la variable de la carne (un 2.25%), por lo que se puede concluir que la calidad de la muestra es aceptable.
# PASO 2: obtenemos muestra con todas las instancias de las clases minoritarias + muestreo clase mayoritaria
df_8_15 <- AP_Final %>% filter(Num_Compras_Totales == "Entre 8 y 15 (ambos incluidos)")
df_16 <- AP_Final %>% filter(Num_Compras_Totales == "Mayor o igual a 16")
df_sample <- bind_rows(sample_df_final, df_8_15,df_16)
# PASO 3: incrementamos las muestras de las clases minoritarias haciendo uso del algoritmo Adasyn
df_equilibrio_cube_adasyn <- AdasynClassif(Num_Compras_Totales ~ ., df_sample,
k=20, # número de vecinos
baseClass = c("Menor o igual a 7"), # identifica la clase mayoritaria
dist = "HEOM") # métrica para tener en cuenta variables numéricas y nominales
formattable(distinct_function_count(df_equilibrio_cube_adasyn, "Num_Compras_Totales"))
# Por la tabla anterior, se demuestra que la muestra queda equilibrada
df_equilibrio_cube_adasyn <- AdasynClassif(Num_Compras_Totales ~ ., df_sample,
k=200, # número de vecinos
baseClass = c("Menor o igual a 7"), # identifica la clase mayoritaria
dist = "HEOM") # métrica para tener en cuenta variables numéricas y nominales
formattable(distinct_function_count(df_equilibrio_cube_adasyn, "Num_Compras_Totales"))
#`ADASYN` y `MUESTREO DEL CUBO` son los dos métodos que vamos a emplear para equilibrar la muestra.
#Elimino l variable ID:
AP_Final$ID <- NULL
#Utilizaremos el muestreo del cubo para obtener una muestra de la clase mayoritaria (**n=877**) y el Adasyn para realizar oversampling sobre las clases minoritarias.
#- Paso 1: undersampling clase mayoritaria mediante el muestreo del cubo
#En primer lugar, hay que tener en cuenta las variables de equilibrio (en este caso las variables cualitativas).
# Datos donde efectuamos selección de las comprar menores o iguales a 7:
df_7 <- AP_Final %>% filter(Num_Compras_Totales == "Menor o igual a 7")
# Número de compras menores o iguales a 7
df_menor_8 <- nrow(df_7)
# Creamos las variables indicadores para cada una de las variables de equilibrio.
# Variable que vale 1 en todas las partes (para comprobar la estimación del tamaño poblacional)
UNO = rep(1, df_menor_8)
# Variables cuantitativas
X1 <- df_7 [ ,c("Year_Birth", "Income", "MntWines", "MntFruits", "MntMeatProducts","MntFishProducts","MntSweetProducts","MntGoldProds","Días_Registro_ult_compra")]
# Variables cualitativas - creación de las variables indicadoras
X2 <- disjunctive(df_7$Education)
colnames(X2) <- levels(df_7$Education)
X3 <- disjunctive(df_7$Marital_Status_woNA)
colnames(X3) <- levels(df_7$Marital_Status_woNA)
X4 <- disjunctive(df_7$Kidhome)
colnames(X4) <- levels(df_7$Kidhome)
X5 <- disjunctive(df_7$Teenhome)
colnames(X5) <- levels(df_7$Teenhome)
X6 <- disjunctive(df_7$Response)
colnames(X6) <- levels(df_7$Response)
# Matriz de diseño
X <- as.matrix( cbind( UNO, X1, X2, X3, X4, X5 ) )
# Tamaño de la muestra
sample_df_7 <- 640
# Probabilidades de inclusión
pik = rep( sample_df_7 / df_menor_8, df_menor_8 )
# extracción de la muestra
set.seed(123)
s = samplecube(X, pik, method = 2, order = 1, comment = FALSE)
# Generación de fichero resultante
sample_df_final = cbind( df_7, s )
sample_df_final <- sample_df_final[ sample_df_final$s == 1, ]
sample_df_final$s  <- NULL
summary(sample_df_final)
#Calidad de la muestra obtenida. Se analiza del siguiente modo:
Totales <- apply(X, 2, sum)
Horvitz.Thompson <- apply(X * s / pik, 2, sum)
calidad <- cbind.data.frame(Totales, Horvitz.Thompson)
calidad$Desv.Abs. <- round(calidad$Totales - calidad$Horvitz.Thompson, 2)
calidad$Desv.Rel. <- round((calidad$Totales / calidad$Horvitz.Thompson - 1) *100, 2)
formattable(calidad)
# La mayor desviaciónn relativa se tiene con la variable de la carne (un 2.25%), por lo que se puede concluir que la calidad de la muestra es aceptable.
# PASO 2: obtenemos muestra con todas las instancias de las clases minoritarias + muestreo clase mayoritaria
df_8_15 <- AP_Final %>% filter(Num_Compras_Totales == "Entre 8 y 15 (ambos incluidos)")
df_16 <- AP_Final %>% filter(Num_Compras_Totales == "Mayor o igual a 16")
df_sample <- bind_rows(sample_df_final, df_8_15,df_16)
# PASO 3: incrementamos las muestras de las clases minoritarias haciendo uso del algoritmo Adasyn
df_equilibrio_cube_adasyn <- AdasynClassif(Num_Compras_Totales ~ ., df_sample,
k=20, # número de vecinos
baseClass = c("Menor o igual a 7"), # identifica la clase mayoritaria
dist = "HEOM") # métrica para tener en cuenta variables numéricas y nominales
formattable(distinct_function_count(df_equilibrio_cube_adasyn, "Num_Compras_Totales"))
# Por la tabla anterior, se demuestra que la muestra queda equilibrada
